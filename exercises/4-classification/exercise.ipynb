{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification on `emnist`\n",
    "\n",
    "## 1. Create `Readme.md` to document your work\n",
    "\n",
    "Explain your choices, process, and outcomes.\n",
    "\n",
    "## 2. Classify all symbols\n",
    "\n",
    "### Choose a model\n",
    "\n",
    "Your choice of model! Choose wisely...\n",
    "\n",
    "### Train away!\n",
    "\n",
    "Is do you need to tune any parameters? Is the model expecting data in a different format?\n",
    "\n",
    "### Evaluate the model\n",
    "\n",
    "Evaluate the models on the test set, analyze the confusion matrix to see where the model performs well and where it struggles.\n",
    "\n",
    "### Investigate subsets\n",
    "\n",
    "On which classes does the model perform well? Poorly? Evaluate again, excluding easily confused symbols (such as 'O' and '0').\n",
    "\n",
    "### Improve performance\n",
    "\n",
    "Brainstorm for improving the performance. This could include trying different architectures, adding more layers, changing the loss function, or using data augmentation techniques.\n",
    "\n",
    "## 2. Classify digits vs. letters model showdown\n",
    "\n",
    "Perform a full showdown classifying digits vs letters:\n",
    "\n",
    "1. Create a column for whether each row is a digit or a letter\n",
    "2. Choose an evaluation metric \n",
    "3. Choose several candidate models to train\n",
    "4. Divide data to reserve a validation set that will NOT be used in training/testing\n",
    "5. K-fold train/test\n",
    "    1. Create train/test splits from the non-validation dataset \n",
    "    2. Train each candidate model (best practice: use the same split for all models)\n",
    "    3. Apply the model the the test split \n",
    "    4. (*Optional*) Perform hyper-parametric search\n",
    "    5. Record the model evaluation metrics\n",
    "    6. Repeat with a new train/test split\n",
    "6. Promote winner, apply model to validation set\n",
    "7. (*Optional*) Perform hyper-parametric search, if applicable\n",
    "8. Report model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work\n",
    "\n",
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (once per virtual environment)\n",
    "%pip install -q numpy pandas matplotlib seaborn scikit-learn tensorflow xgboost scikeras\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cg/9wys6fms7x5096b1wd70r9f40000gn/T/ipykernel_27788/856261851.py:6: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import emnist\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML packages\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold, cross_validate\n",
    "# XGBoost (SVM)\n",
    "from xgboost import XGBClassifier\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# Keras formatting helper\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SIZE = 28\n",
    "REBUILD = True\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # just show ERROR & FATAL warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def int_to_char(label):\n",
    "    \"\"\"Convert an integer label to the corresponding uppercase character. Using Unicode\"\"\"\n",
    "    if label < 10:\n",
    "        return str(label)\n",
    "    elif label < 36:\n",
    "        return chr(label - 10 + ord('A'))\n",
    "    else:\n",
    "        return chr(label - 36 + ord('a'))\n",
    "    \n",
    "def int_to_type(label):\n",
    "    \"\"\"Convert an integer label to type (letter/digit)\"\"\"\n",
    "    if label < 10:\n",
    "        return 'digit'\n",
    "    else:\n",
    "        return 'letter'\n",
    "    \n",
    "def class_to_int(emnist_classes): \n",
    "    \"\"\"Define a function that takes a class and returns the integer label\"\"\"\n",
    "    class_list = list(string.digits + string.ascii_uppercase + string.ascii_lowercase)\n",
    "    label=[]\n",
    "    for i in emnist_classes: \n",
    "        label.append(class_list.index(i))\n",
    "    return label\n",
    "\n",
    "def plot_accuracy(history):\n",
    "    \"\"\"Plot the training and validation accuracy during the training of a model.\"\"\"\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(history):\n",
    "    \"\"\"Plot the training and validation loss during the training of a model.\"\"\"\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# This is a common preprocessing step for neural networks, but may not be necessary in all cases\n",
    "def normalize_images(images):\n",
    "    \"\"\"Normalize the pixel values of the images in the dataset to have zero mean and unit variance.\"\"\"\n",
    "    images = np.array(images)\n",
    "    mean = images.mean()\n",
    "    std = images.std()\n",
    "    images = (images - mean) / std\n",
    "    return images.tolist()\n",
    "\n",
    "# exercise 1\n",
    "def display_metrics_symbols(task, model_name, metrics_dict):\n",
    "    \"\"\"Display performance metrics and confusion matrix for symbol models.\"\"\"\n",
    "    metrics_df = pd.DataFrame()\n",
    "    cm_df = pd.DataFrame()\n",
    "    for key, value in metrics_dict[task][model_name].items():\n",
    "        if type(value) == np.ndarray:\n",
    "            class_lab = list(string.digits + string.ascii_uppercase + string.ascii_lowercase)\n",
    "            cm_df = pd.DataFrame(value, index=['actual {}'.format(i) for i in class_lab], columns=['predict {}'.format(i) for i in class_lab])\n",
    "        else:\n",
    "            metrics_df[key] = [value]\n",
    "    display(Markdown(f'# Performance Metrics: {model_name}'))\n",
    "    display(metrics_df)\n",
    "    display(Markdown(f'# Confusion Matrix: {model_name}'))\n",
    "    display(cm_df)\n",
    "\n",
    "# Define a function that takes row names and a labeled confusion matrix as input to generate a table of top classification classes \n",
    "def top_classes(row_names, source_df, class_n=5):\n",
    "    \"\"\"display top 5 classificaition classes & numbers for symbols listed \n",
    "    row_names = list of row names in the format of ['actual 1', 'actual A', 'actual a']\n",
    "    source_df = labeled confusion matrix\n",
    "    class_n = number of top classification classes, default set to 5\n",
    "    \"\"\"\n",
    "    class_df = pd.DataFrame()\n",
    "    for actuals in row_names: \n",
    "        display(Markdown(f'## Top {class_n} classificaition classes for {actuals}'))\n",
    "        class_df['Class'] = source_df.loc[actuals,].sort_values(ascending=False).nlargest(class_n).index.tolist()\n",
    "        class_df['Number'] = source_df.loc[actuals,].sort_values(ascending=False).nlargest(class_n).tolist()\n",
    "        class_df['Percent(%)'] = class_df['Number']/sum(source_df.loc[actuals,])*100\n",
    "        display(class_df)\n",
    "\n",
    "# exercise 2\n",
    "def display_metrics_type(task, model_name, metrics_dict):\n",
    "    \"\"\"Display performance metrics and confusion matrix for type models.\"\"\"\n",
    "    metrics_df = pd.DataFrame()\n",
    "    cm_df = pd.DataFrame()\n",
    "    for key, value in metrics_dict[task][model_name].items():\n",
    "        if type(value) == np.ndarray:\n",
    "            cm_df = pd.DataFrame(value, index=['actual digit', 'actual letter'], columns=['predicted digit', 'predicted letter'])\n",
    "        else:\n",
    "            metrics_df[key] = [value]\n",
    "    display(Markdown(f'#### Performance Metrics: {model_name}'))\n",
    "    display(metrics_df)\n",
    "    display(Markdown(f'#### Confusion Matrix: {model_name}'))\n",
    "    display(cm_df)\n",
    "\n",
    "def display_candidate_model_metrics(task, candidate_metrics_dict):\n",
    "    \"\"\"Display performance metrics candidate models trained by 3-fold cv and generate an average table to evaluate models.\"\"\"\n",
    "    metrics_df = pd.DataFrame()\n",
    "    metric_means_df = pd.DataFrame()\n",
    "    for model_name in candidate_metrics_dict[task]:\n",
    "        metrics_df = pd.DataFrame(candidate_metrics_dict[task][model_name], index=['CV1', 'CV2', 'CV3']).T\n",
    "        metrics_df['mean'] = metrics_df.mean(axis=1)\n",
    "        metric_means_df[model_name] = metrics_df.mean(axis=1)\n",
    "        display(Markdown(f'#### Performance Metrics: {model_name}'))\n",
    "        display(metrics_df)\n",
    "    display(Markdown(f'#### Summary Performance Metrics'))\n",
    "    display(metric_means_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class         20379\n",
       "image         20379\n",
       "image_flat    20379\n",
       "label         20379\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load training data\n",
    "image, label = emnist.extract_training_samples('byclass')\n",
    "train = pd.DataFrame()\n",
    "train['class'] = np.array([int_to_char(l) for l in label])\n",
    "train['image'] = list(image) \n",
    "train['image_flat'] = train['image'].apply(lambda x: np.array(x).reshape(-1)) \n",
    "train['label'] = label\n",
    "\n",
    "# Subset training data\n",
    "train = train[train['class'].isin(['A', 'B', 'C','D','E','F','G'])]\n",
    "\n",
    "# Count \n",
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class         3449\n",
       "image         3449\n",
       "image_flat    3449\n",
       "label         3449\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load testing data\n",
    "image, label = emnist.extract_test_samples('byclass')\n",
    "valid = pd.DataFrame()\n",
    "valid['class'] = np.array([int_to_char(l) for l in label])\n",
    "valid['image'] = list(image) # 28*28 array\n",
    "valid['image_flat'] = valid['image'].apply(lambda x: np.array(x).reshape(-1)) # array with length: 784\n",
    "valid['label'] = label\n",
    "\n",
    "# Subset testing data\n",
    "valid = valid[valid['class'].isin(['A', 'B', 'C','D','E','F','G'])]\n",
    "\n",
    "# Count \n",
    "valid.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Classify all symbols \n",
    "### 1.a Choose a model\n",
    "Classification will be done using Neural Network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all the metrics in a dictionary\n",
    "metrics_dict = {\n",
    "    'Classify_all_symbols' : { \n",
    "        'neural_network': {\n",
    "            'confusion_matrix': [],\n",
    "            'accuracy': [],\n",
    "            'precision': [],\n",
    "            'recall': [],\n",
    "            'f1': []\n",
    "        }\n",
    "    }, \n",
    "    'Classify_all_symbols_validation_with_selected_data' : { \n",
    "        'neural_network': {\n",
    "            'confusion_matrix': [],\n",
    "            'accuracy': [],\n",
    "            'precision': [],\n",
    "            'recall': [],\n",
    "            'f1': []\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment if using whole dataset to train \n",
    "#train = train[:10000]\n",
    "#print('Number of unique class values: ', len(train['class'].unique()))\n",
    "\n",
    "# Comment if using whole dataset to validate \n",
    "#valid = valid[:3000]\n",
    "#print('Number of unique class values: ', len(valid['class'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "task = 'Classify_all_symbols from A to G'\n",
    "model_name = 'neural_network'\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Convert data to tensor\n",
    "train_images = np.array(train['image'])\n",
    "train_images = np.array(list(map(lambda x: np.reshape(x, (28, 28, 1)), train_images)))\n",
    "train_images = train_images / 255.0\n",
    "train_labels = np.array(train['label'])\n",
    "valid_images = np.array(valid['image'])\n",
    "valid_images = np.array(list(map(lambda x: np.reshape(x, (28, 28, 1)), valid_images)))\n",
    "valid_images = valid_images / 255.0\n",
    "valid_labels = np.array(valid['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize neural network model\n",
    "model = Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(62, activation='softmax')\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model, specifying the optimizer, loss function, and metrics \n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, saving the history of the training process\n",
    "history = model.fit(train_images, train_labels, epochs=10, batch_size=3500, validation_data=(valid_images, valid_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c Evaluate the model\n",
    "#### Overall evalation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, acc = model.evaluate(valid_images, valid_labels)\n",
    "y_pred = np.argmax(model.predict(valid_images), axis=1)\n",
    "\n",
    "# Calculate performance metrics\n",
    "prec = precision_score(valid_labels, y_pred, average='weighted')\n",
    "rec = recall_score(valid_labels, y_pred, average='weighted')\n",
    "f1 = f1_score(valid_labels, y_pred, average='weighted')\n",
    "cm = confusion_matrix(valid_labels, y_pred)\n",
    "\n",
    "# Store performance metrics in dictionary\n",
    "metrics_dict[task][model_name] = {'accuracy': acc,\n",
    "                                  'precision': prec,\n",
    "                                  'recall': rec,\n",
    "                                  'f1': f1,\n",
    "                                  'confusion_matrix': cm}\n",
    "\n",
    "# Display performance metrics\n",
    "display_metrics_symbols(task, model_name, metrics_dict)\n",
    "\n",
    "# Plot the training and validation accuracy during the training of the model\n",
    "plot_accuracy(history)\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.d Investigate subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_lab = list(string.digits + string.ascii_uppercase + string.ascii_lowercase)\n",
    "class_report = classification_report(valid_labels, y_pred, target_names=class_lab, zero_division=0)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of symbols with lowest recall`lowest_recall_char_rowname`\n",
    "recall_scores_nn = recall_score(valid_labels, y_pred, average=None)\n",
    "lowest_recall_char = [int_to_char(i) for i in np.argsort(recall_scores_nn)]\n",
    "lowest_recall_char_rowname = ['actual {}'.format(i) for i in lowest_recall_char]\n",
    "\n",
    "# create cm pandas dataframe \n",
    "cm_df = pd.DataFrame(cm, index=['actual {}'.format(i) for i in class_lab], columns=['pred {}'.format(i) for i in class_lab])\n",
    "\n",
    "# Find top classification classes for n number of symbols with lowest recall \n",
    "n=15\n",
    "display(top_classes(lowest_recall_char_rowname[0:n], cm_df))\n",
    "# Specifying symbol and number of classification classes\n",
    "display(top_classes(['actual m'], cm_df, class_n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate again, excluding easily confused symbols (such as 'O' and '0'): \n",
    "\n",
    "\n",
    "- Results show that accuracy increased by only including those that are easy to classify. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-evaluation: Exclude easily confused symbols\n",
    "\n",
    "task = 'Classify_all_symbols_validation_with_selected_data'\n",
    "model_name = 'neural_network'\n",
    "\n",
    "# Exclude easily confused symbols in validation data \n",
    "excluded = class_to_int(list('0oOiI1lZ2z'))\n",
    "mask_valid = valid['label'].apply(lambda x: x not in excluded)\n",
    "valid_nonconfused = valid[mask_valid]\n",
    "\n",
    "# Convert subset validation data to tensor\n",
    "nonconfused_images = np.array(valid_nonconfused['image'])\n",
    "nonconfused_images = np.array(list(map(lambda x: np.reshape(x, (28, 28, 1)), nonconfused_images)))\n",
    "nonconfused_images = nonconfused_images / 255.0\n",
    "nonconfused_labels = np.array(valid_nonconfused['label'])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, acc = model.evaluate(nonconfused_images, nonconfused_labels)\n",
    "nonconfused_pred = np.argmax(model.predict(nonconfused_images), axis=1)\n",
    "\n",
    "# Calculate performance metrics\n",
    "prec = precision_score(nonconfused_labels, nonconfused_pred, average='weighted')\n",
    "rec = recall_score(nonconfused_labels, nonconfused_pred, average='weighted')\n",
    "f1 = f1_score(nonconfused_labels, nonconfused_pred, average='weighted')\n",
    "cm = confusion_matrix(nonconfused_labels, nonconfused_pred)\n",
    "\n",
    "# Store performance metrics in dictionary\n",
    "metrics_dict[task][model_name] = {'accuracy': acc,\n",
    "                                  'precision': prec,\n",
    "                                  'recall': rec,\n",
    "                                  'f1': f1,\n",
    "                                  'confusion_matrix': cm}\n",
    "\n",
    "# Make confusion matrix indices more readable\n",
    "cm_labels = []\n",
    "for i in np.unique(nonconfused_pred).tolist(): \n",
    "    cm_labels.append(int_to_char(i))\n",
    "cm_labels\n",
    "\n",
    "#Display performance metrics and confusion matrix for a model.\n",
    "metrics_df = pd.DataFrame()\n",
    "cm_df = pd.DataFrame()\n",
    "for key, value in metrics_dict[task][model_name].items():\n",
    "    if type(value) == np.ndarray:\n",
    "        class_lab = list(string.digits + string.ascii_uppercase + string.ascii_lowercase)\n",
    "        cm_df = pd.DataFrame(value, index=['actual {}'.format(i) for i in cm_labels], columns=['predict {}'.format(i) for i in cm_labels])\n",
    "    else:\n",
    "        metrics_df[key] = [value]\n",
    "display(Markdown(f'# Performance Metrics: {model_name}'))\n",
    "display(metrics_df)\n",
    "display(Markdown(f'# Confusion Matrix: {model_name}'))\n",
    "display(cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.e Improve performance\n",
    "Since we know that there are sets of symbols that are commonly confused such as `['1', 'l', 'I', 'i']` or `['o', 'O', 'O']`, we can use ensemble methods with a few model that specializes in differentiating between commonly misclassified symbols sets, and another overall model that handles the rest of the classification task. The final prediction could be a combination of the predictions from both models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classify digits vs. letters model showdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a column for whether each row is a digit or a letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 'training/testing' data\n",
    "image, label = emnist.extract_training_samples('byclass')\n",
    "train = pd.DataFrame()\n",
    "train['class'] = np.array([int_to_char(l) for l in label])\n",
    "train['image'] = list(image) \n",
    "train['image_flat'] = train['image'].apply(lambda x: np.array(x).reshape(-1)) \n",
    "train['label'] = label\n",
    "# Add a column with the type corresponding digit/letter\n",
    "train['type'] = train['label'].apply(lambda x: int_to_type(x))\n",
    "# Add a column 'isletter' with 0=digit, 1=letter\n",
    "train['isletter']= train['label'].apply(lambda x: 0 if x<10 else 1)\n",
    "\n",
    "\n",
    "# Load testing data\n",
    "image, label = emnist.extract_test_samples('byclass')\n",
    "valid = pd.DataFrame()\n",
    "valid['class'] = np.array([int_to_char(l) for l in label])\n",
    "valid['image'] = list(image) # 28*28 array\n",
    "valid['image_flat'] = valid['image'].apply(lambda x: np.array(x).reshape(-1)) # array with length: 784\n",
    "valid['label'] = label\n",
    "# Add a column with the type corresponding digit/letter\n",
    "valid['type'] = valid['label'].apply(lambda x: int_to_type(x))\n",
    " # Add a column 'isletter' with 0=digit, 1=letter\n",
    "valid['isletter']= valid['label'].apply(lambda x: 0 if x<10 else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Choose an evaluation metric \n",
    "### 3. Choose several candidate models to train\n",
    "both specified in matrix dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_metrics_dict = {\n",
    "    'digit_vs_letter' : { \n",
    "        'logistic_regression': {\n",
    "            'accuracy': [],\n",
    "            'precision': [],\n",
    "            'recall': [],\n",
    "            'f1': []\n",
    "        },\n",
    "        'xgboost': {\n",
    "            'accuracy': [],\n",
    "            'precision': [],\n",
    "            'recall': [],\n",
    "            'f1': []\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'accuracy': [],\n",
    "            'precision': [],\n",
    "            'recall': [],\n",
    "            'f1': []\n",
    "        },\n",
    "        'neural_network': {\n",
    "            'accuracy': [],\n",
    "            'precision': [],\n",
    "            'recall': [],\n",
    "            'f1': []\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Divide data to reserve a validation set that will NOT be used in training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Merge data and subset out\"\"\"\n",
    "merged = pd.concat([train, valid], axis=0, ignore_index=True)\n",
    "\n",
    "# Group by class\n",
    "grouped = merged.groupby('class')\n",
    "# Number of samples from each group\n",
    "n_sample=round(len(merged)/10/62, None)\n",
    "# Sample a subset from each group\n",
    "merged_sampled = grouped.apply(lambda x: x.sample(n_sample, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "# Display number samples \n",
    "print('Number of samples in `merged_sampled`: ', len(merged_sampled))\n",
    "# Check that all 62 classes are present\n",
    "print('Number of unique class values: ', len(merged_sampled['class'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data to reserve a validation set that will NOT be used in training/testing\n",
    "shuffled_df = merged_sampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# Create a 7:3 train:validation data split\n",
    "split_index = round(len(shuffled_df)*0.7, None)\n",
    "train = shuffled_df[:split_index]\n",
    "valid = shuffled_df[split_index:]\n",
    "\n",
    "# Check that all 62 classes are present\n",
    "# In `train` (70% of the data)\n",
    "print('Number of samples in `train`: ', len(train))\n",
    "print('Number of unique class values: ', len(train['class'].unique()))\n",
    "# In `valid` (30% of the data)\n",
    "print('Number of samples in `valid`: ', len(valid))\n",
    "print('Number of unique class values: ', len(valid['class'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. K-fold train/test\n",
    "        1. Create train/test splits from the non-validation dataset \n",
    "        2. Train each candidate model (best practice: use the same split for all models)\n",
    "        3. Apply the model the the test split \n",
    "        4. (*Optional*) Perform hyper-parametric search\n",
    "        5. Record the model evaluation metrics\n",
    "        6. Repeat with a new train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole/subset data selection\n",
    "\n",
    "# Comment for bigger model building\n",
    "#train_subset = train[0:10000]\n",
    "#valid_subset = valid[0:2000]\n",
    "\n",
    "# Comment for small model building \n",
    "train_subset = train\n",
    "valid_subset = valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test splits from the non-validation dataset \n",
    "# Initialize k-fold cross-validation: Create train/test splits from `train_subset`, use this cross-validaiton split for all models \n",
    "kfold = KFold(n_splits=3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# digit_vs_letter Classifier: RandomForest\n",
    "task = 'digit_vs_letter'\n",
    "model_name = 'random_forest'\n",
    "\n",
    "\n",
    "# Initialize random forest classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train and evaluate model: perform k-fold cross-validation \n",
    "cvscore = cross_validate(rf_clf, \n",
    "                         train_subset['image_flat'].tolist(), \n",
    "                         train_subset['isletter'], \n",
    "                         scoring=('accuracy', 'precision', 'recall', 'f1'), \n",
    "                         cv=kfold, n_jobs=-2, return_indices=False)\n",
    "acc = cvscore['test_accuracy']\n",
    "prec = cvscore['test_precision']\n",
    "rec = cvscore['test_recall']\n",
    "f1 = cvscore['test_f1']\n",
    "\n",
    "# Store performance metrics in dictionary\n",
    "candidate_metrics_dict[task][model_name] = {'accuracy': acc,\n",
    "                                            'precision': prec,\n",
    "                                            'recall': rec,\n",
    "                                            'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# digit_vs_letter Classifier: Logistic Regression \n",
    "task = 'digit_vs_letter'\n",
    "model_name = 'logistic_regression'\n",
    "\n",
    "# Initialize logistic regression classifier\n",
    "lr_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "# When running without scaling the data, the model does not converge\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_subset['image_flat'].tolist())\n",
    "\n",
    "# Initialize logistic regression classifier\n",
    "lr_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Train and evaluate model: perform k-fold cross-validation \n",
    "cvscore = cross_validate(lr_clf, \n",
    "                         train_scaled, \n",
    "                         train_subset['isletter'], \n",
    "                         scoring=('accuracy', 'precision', 'recall', 'f1'), \n",
    "                         cv=kfold, n_jobs=-2, return_indices=False)\n",
    "acc = cvscore['test_accuracy']\n",
    "prec = cvscore['test_precision']\n",
    "rec = cvscore['test_recall']\n",
    "f1 = cvscore['test_f1']\n",
    "\n",
    "# Store performance metrics in dictionary\n",
    "candidate_metrics_dict[task][model_name] = {'accuracy': acc,\n",
    "                                            'precision': prec,\n",
    "                                            'recall': rec,\n",
    "                                            'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# digit_vs_letter Classifier: XGBoost\n",
    "task = 'digit_vs_letter'\n",
    "model_name = 'xgboost'\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "xgb_clf = XGBClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train and evaluate model: perform k-fold cross-validation \n",
    "cvscore = cross_validate(xgb_clf, \n",
    "                         train_subset['image_flat'].tolist(), \n",
    "                         train_subset['isletter'], \n",
    "                         scoring=('accuracy', 'precision', 'recall', 'f1'), \n",
    "                         cv=kfold, n_jobs=-2, return_indices=False)\n",
    "acc = cvscore['test_accuracy']\n",
    "prec = cvscore['test_precision']\n",
    "rec = cvscore['test_recall']\n",
    "f1 = cvscore['test_f1']\n",
    "\n",
    "# Store performance metrics in dictionary\n",
    "candidate_metrics_dict[task][model_name] = {'accuracy': acc,\n",
    "                                            'precision': prec,\n",
    "                                            'recall': rec,\n",
    "                                            'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# digit vs letter Classifier: Neural Network\n",
    "task = 'digit_vs_letter'\n",
    "model_name = 'neural_network'\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Convert data to tensor\n",
    "train_images = np.array(train_subset['image'])\n",
    "train_images = np.array(list(map(lambda x: np.reshape(x, (28, 28, 1)), train_images)))\n",
    "train_images = train_images / 255.0\n",
    "train_type = np.array(train_subset['isletter'])\n",
    "\n",
    "# Initialize neural network model\n",
    "def build_nn_model(): \n",
    "    model = Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dense(1, activation='sigmoid')])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model \n",
    "\n",
    "# wrap the Keras model inside a scikit-learn compatible wrapper \n",
    "keras_wrapped_nnmodel = KerasClassifier(build_fn=build_nn_model, epochs=5, verbose=1)\n",
    "\n",
    "# Train and evaluate model: perform k-fold cross-validation \n",
    "cvscore = cross_validate(keras_wrapped_nnmodel, train_images, train_type,\n",
    "                         scoring=('accuracy', 'precision', 'recall', 'f1'), \n",
    "                         cv=kfold, verbose=1, return_indices=False)\n",
    "\n",
    "acc = cvscore['test_accuracy']\n",
    "prec = cvscore['test_precision']\n",
    "rec = cvscore['test_recall']\n",
    "f1 = cvscore['test_f1']\n",
    "\n",
    "# Store performance metrics in dictionary\n",
    "candidate_metrics_dict[task][model_name] = {'accuracy': acc,\n",
    "                                            'precision': prec,\n",
    "                                            'recall': rec,\n",
    "                                            'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_candidate_model_metrics(task, candidate_metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Promote winner, apply model to validation set: \n",
    "**Random forest model** has the <u>highest recall<u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit data on validation set\n",
    "\n",
    "# digit_vs_letter Classifier: RandomForest\n",
    "task = 'digit_vs_letter_final_model'\n",
    "model_name = 'random_forest'\n",
    "\n",
    "\n",
    "# Initialize random forest classifier\n",
    "rf_cl_dvl = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train and generate predictions\n",
    "rf_cl_dvl.fit(train_subset['image_flat'].tolist(), train_subset['isletter'])\n",
    "y_pred = rf_cl_dvl.predict(valid_subset['image_flat'].tolist())\n",
    "\n",
    "# Evaluate model\n",
    "acc = accuracy_score(valid_subset['isletter'], y_pred)\n",
    "prec = precision_score(valid_subset['isletter'], y_pred)\n",
    "rec = recall_score(valid_subset['isletter'], y_pred)\n",
    "f1 = f1_score(valid_subset['isletter'], y_pred)\n",
    "cm = confusion_matrix(valid_subset['isletter'], y_pred)\n",
    "\n",
    "# Store evaluation metrics in dictionary \n",
    "final_model_metrics = {task:{model_name:{}}}\n",
    "final_model_metrics[task][model_name] = {'accuracy': acc,\n",
    "                                         'precision': prec,\n",
    "                                         'recall': rec,\n",
    "                                         'f1': f1,\n",
    "                                         'confusion_matrix': cm}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. (*Optional*) Perform hyper-parametric search, if applicable\n",
    "N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Report model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display performance metrics and confusion matrix for a model.\n",
    "metrics_df = pd.DataFrame()\n",
    "cm_df = pd.DataFrame()\n",
    "for key, value in final_model_metrics[task][model_name].items():\n",
    "    if type(value) == np.ndarray:\n",
    "        cm_df = pd.DataFrame(value, index=['actual digit', 'actual letter'], columns=['predicted digit', 'predicted letter'])\n",
    "    else:\n",
    "        metrics_df[key] = [value]\n",
    "display(Markdown(f'# Performance Metrics: {model_name}'))\n",
    "display(metrics_df)\n",
    "display(Markdown(f'# Confusion Matrix: {model_name}'))\n",
    "display(cm_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".datasci_223",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
